{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtu22D+ygGDHkin7KT0ADC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivamThapa243/Information-Retrieval/blob/main/unigram_inverted_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNIGRAM INVERTED INDEX\n",
        "\n"
      ],
      "metadata": {
        "id": "jyfiLkRMO8B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. Building Unigram Inverted Index**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZ1G8EOl-_QW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7eGwA0lqOwVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b51f7ed-20ad-451b-afa0-350c7e4462ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# importing files from drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to build an unigram inverted index\n",
        "\n",
        "Structure of inverted index:\n",
        "*   word1: {counts: x, documents: [doc1, dox2, doc3...]}\n",
        "*   word2: {counts: y, documents: [doc3, doc4, doc5...]}\n",
        "*   ...\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "j7t4dLlkaMZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def unigram_inverted_index_builder(dataset_directory):\n",
        "  unigram_inverted_index = {}\n",
        "  list_file = os.listdir(dataset_directory)\n",
        "\n",
        "  # Iterating through each file present in the directory\n",
        "  for filename in list_file:\n",
        "    if filename.endswith(\".txt\"):\n",
        "      # Reading the content of the file\n",
        "      file_path = os.path.join(dataset_directory, filename)\n",
        "      with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "      # tokenizing the documnet to get unique tokens\n",
        "      content_list = content.split()\n",
        "      unique_content = set(content_list)\n",
        "\n",
        "      # Updating the unigram inverted index\n",
        "      for token in unique_content:\n",
        "        # if the token is already in the inverted_index\n",
        "        if token in unigram_inverted_index:\n",
        "          unigram_inverted_index[token]['count'] += 1\n",
        "          if filename not in unigram_inverted_index[token]['documents']:\n",
        "            unigram_inverted_index[token]['documents'].append(filename)\n",
        "        else:\n",
        "          unigram_inverted_index[token] = {'count' : 1, 'documents': [filename]}\n",
        "  return unigram_inverted_index\n"
      ],
      "metadata": {
        "id": "LduAmbNE6wFK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoking the unigram_inverted_index_builder function to build the inverted index"
      ],
      "metadata": {
        "id": "LmVXAr25nggM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# location of preprocessed data set which is stored in the google drive\n",
        "dataset_directory = \"/content/drive/MyDrive/Information Retrieval/preprocessed_data\"\n",
        "\n",
        "# calling the builder function by passing the data set location\n",
        "unigram_inverted_index = unigram_inverted_index_builder(dataset_directory)\n",
        "\n",
        "# storing the newely generated unigram inverted index into a new text file\n",
        "directory_name = \"/content/drive/MyDrive/Information Retrieval\"\n",
        "text_file_name = \"unigram_inverted_index_dataset.txt\"\n",
        "text_file = os.path.join(directory_name, text_file_name)\n",
        "\n",
        "with open(text_file, 'w') as file:\n",
        "  for term, info in unigram_inverted_index.items():\n",
        "    file.write(f\"{term}: {info}\\n\")\n",
        "\n",
        "print(\"Unigram inverted index created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUzZjL0vY2Ar",
        "outputId": "40c67a17-d4eb-4b0d-d81a-2f2c47f765f3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram inverted index created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting the inverted index"
      ],
      "metadata": {
        "id": "vmy-QBX0A_vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sorting the earlier generated unigram inverted index\n",
        "sorted_items = sorted(unigram_inverted_index.items(), key = lambda x : x[0])\n",
        "sorted_inverted_index = dict(sorted_items)\n",
        "\n",
        "# storing the sorted inverted index into a new text file\n",
        "sorted_text_file_name = \"sorted_unigram_inverted_index_dataset.txt\"\n",
        "sorted_text_file = os.path.join(directory_name, sorted_text_file_name)\n",
        "\n",
        "with open(sorted_text_file, 'w') as file:\n",
        "  for term, info in sorted_inverted_index.items():\n",
        "    file.write(f\"{term} : {info}\\n\")\n",
        "\n",
        "print(\"Sorted unigram inverted index created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL35zl0g_6Ho",
        "outputId": "aef5bd6d-e8b2-4486-bb57-9d9b3ec5f541"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted unigram inverted index created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Pickling the Unigram Inverted Index**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8sBWBqDO_Q5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pkl_file_name = \"sorted_unigram_inverted_index.pkl\"\n",
        "pkl_file_path = os.path.join(directory_name, pkl_file_name)\n",
        "\n",
        "with open(pkl_file_path, 'wb') as file:\n",
        "  pickle.dump(sorted_inverted_index, file)\n",
        "\n",
        "print(\"Sorted unigram inverted index pickled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYQXVUG-FFtb",
        "outputId": "a8cfbf9a-c24a-45e7-8068-05e4853e507e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted unigram inverted index pickled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Providing support for the query operations:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   T1 **AND** T2\n",
        "2.   T1 **OR** T2\n",
        "3.   T1 **AND NOT** T2\n",
        "4.   T1 **OR NOT** T2\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3bXVqClHxTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to preprocess the query and return a preprocessed result"
      ],
      "metadata": {
        "id": "p529pAb3LW8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocessing(query):\n",
        "  # lower case the query\n",
        "  query = query.lower()\n",
        "\n",
        "  # tokenizing\n",
        "  tokens = word_tokenize(query)\n",
        "\n",
        "  # punctuation removal\n",
        "  tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "  # stopwords removal\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxL4tY_wLLnn",
        "outputId": "2c5df109-0036-4863-abed-af9724bf2532"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuction to perform operation on query"
      ],
      "metadata": {
        "id": "Pn-bmphVexkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def sorted_posting_list(posting_list):\n",
        "  # posting_list = ['file1.txt', 'file2.txt', 'file3.txt']\n",
        "  # numeric part is needed\n",
        "\n",
        "  pattern = re.compile(r'\\d+')\n",
        "  # storing the numeric part\n",
        "  numeric_list = []\n",
        "\n",
        "  for posting in posting_list:\n",
        "    matches = pattern.search(posting)\n",
        "    if matches:\n",
        "      numeric_part = int(matches.group())\n",
        "      numeric_list.append(numeric_part)\n",
        "    else:\n",
        "      numeric_list.append(0)\n",
        "\n",
        "  numeric_list.sort()\n",
        "  return numeric_list\n",
        "\n",
        "\n",
        "def intersection_operation(posting_list_1, posting_list_2):\n",
        "  list_1 = sorted_posting_list(posting_list_1)\n",
        "  list_2 = sorted_posting_list(posting_list_2)\n",
        "\n",
        "  # itersecting only the\n",
        "  intersected_list = []\n",
        "\n",
        "  pointer_1 = 0\n",
        "  pointer_2 = 0\n",
        "  while (pointer_1 < len(list_1) and pointer_2 < len(list_2)):\n",
        "    if list_1[pointer_1] == list_2[pointer_2]:\n",
        "      intersected_list.append(list_1[pointer_1])\n",
        "      pointer_1 += 1\n",
        "      pointer_2 += 1\n",
        "    elif list_1[pointer_1] > list_2[pointer_2]:\n",
        "      pointer_2 += 1\n",
        "    else:\n",
        "      pointer_1 += 1\n",
        "  return intersected_list\n",
        "# end of intersection function\n",
        "\n",
        "# union function\n",
        "def union_operation(posting_list_1, posting_list_2):\n",
        "  list_1 = sorted_posting_list(posting_list_1)\n",
        "  list_2 = sorted_posting_list(posting_list_2)\n",
        "\n",
        "  # itersecting only the\n",
        "  unioned_list = []\n",
        "\n",
        "  pointer_1 = 0\n",
        "  pointer_2 = 0\n",
        "\n",
        "  while pointer_1 < len(list_1) and pointer_2 < len(list_2):\n",
        "    if list_1[pointer_1] == list_2[pointer_2]:\n",
        "      unioned_list.append(list_1[pointer_1])\n",
        "      pointer_1 += 1\n",
        "      pointer_2 +=1\n",
        "    elif list_1[pointer_1] > list_2[pointer_2]:\n",
        "      unioned_list.append(list_2[pointer_2])\n",
        "      pointer_2 += 1\n",
        "    else:\n",
        "      unioned_list.append(list_1[pointer_1])\n",
        "      pointer_1 += 1\n",
        "\n",
        "  if pointer_1 < len(list_1):\n",
        "    unioned_list.extend(list_1[pointer_1:])\n",
        "  if pointer_2 < len(list_2):\n",
        "    unioned_list.extend(list_2[pointer_2:])\n",
        "\n",
        "  return unioned_list\n",
        "# end of union function"
      ],
      "metadata": {
        "id": "kfeoYFfEe2r8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Point: To check the working of operations on posting list"
      ],
      "metadata": {
        "id": "RM5yaMp_KIFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posting1 = ['file542.txt', 'file174.txt', 'file264.txt', 'file746.txt', 'file886.txt', 'file166.txt']\n",
        "posting2 = ['file797.txt', 'file956.txt', 'file174.txt', 'file942.txt', 'file73.txt', 'file892.txt', 'file698.txt', 'file981.txt', 'file313.txt', 'file780.txt', 'file682.txt', 'file738.txt', 'file864.txt', 'file863.txt', 'file573.txt', 'file860.txt', 'file459.txt', 'file404.txt', 'file886.txt', 'file3.txt', 'file118.txt', 'file686.txt', 'file699.txt', 'file466.txt', 'file665.txt', 'file363.txt', 'file930.txt']\n",
        "\n",
        "numeric_list1 = sorted_posting_list(posting1)\n",
        "numeric_list2 = sorted_posting_list(posting2)\n",
        "print(\"Sorted numeric posting list 1: \")\n",
        "print(numeric_list1)\n",
        "print(\"\\nSorted numeric posting list 2: \")\n",
        "print(numeric_list2)\n",
        "\n",
        "intersect_list = intersection_operation(posting1, posting2)\n",
        "print(\"\\nIntersection :\")\n",
        "print(intersect_list)\n",
        "\n",
        "print(\"\\nUnion : \")\n",
        "union_list = union_operation(posting1, posting2)\n",
        "print(union_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYEjIOAD7DnA",
        "outputId": "2fe324dc-25ae-4463-9afb-e87f7f393f29"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted numeric posting list 1: \n",
            "[166, 174, 264, 542, 746, 886]\n",
            "\n",
            "Sorted numeric posting list 2: \n",
            "[3, 73, 118, 174, 313, 363, 404, 459, 466, 573, 665, 682, 686, 698, 699, 738, 780, 797, 860, 863, 864, 886, 892, 930, 942, 956, 981]\n",
            "\n",
            "Intersection :\n",
            "[174, 886]\n",
            "\n",
            "Union : \n",
            "[3, 73, 118, 166, 174, 264, 313, 363, 404, 459, 466, 542, 573, 665, 682, 686, 698, 699, 738, 746, 780, 797, 860, 863, 864, 886, 892, 930, 942, 956, 981]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    ***Algorithm: (Not based on the length of the posting list)***\n",
        "    #   implementing list as queue:\n",
        "    #   query_tokens : [word1, word2, word3, word4]\n",
        "    #   operator_list : [operator1, operator2, operator3]\n",
        "    1.  creating a new list which will store the posting lists of the corresponding words\n",
        "          query_postings = [posting_list_word_1, posting_list_word_2, posting_list_word_3, posting_list_word_4]\n",
        "    2.  while operator_list is not empty:\n",
        "    2.  perfomring the operation on query_postings[0] and query_postings[1] with operator_list[0] ALWAYS\n",
        "          result_posting_list = posting_list_function(query_tokens[0], query_tokens[1], operators[0])\n",
        "    3.  Pop out the query_postngs[0] and query_postings[1] as their work is done\n",
        "    4.  pop out the operator_list[0] as well as its work is also done\n",
        "    5.  Storing the resulted posting list at the 0th index of the query_postings[]"
      ],
      "metadata": {
        "id": "k4AQGwcWEmkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def operations_on_query(query_tokens, operator_list, inverted_index):\n",
        "  # list to store the posting lists of corresponding query tokens (list of lists)\n",
        "  posting_list = []\n",
        "  for token in query_tokens:\n",
        "    if token in inverted_index:\n",
        "      posting_list.append(inverted_index[token]['documents'])\n",
        "    else:\n",
        "      posting_list.append([])\n",
        "\n",
        "  # check point: printing token and their posting list\n",
        "  for token, documents in zip(query_tokens, posting_list):\n",
        "    print(token, \": \", documents)\n",
        "\n",
        "  # performing operation on posting list and inverted index\n",
        "  while(len(operator_list) > 0):\n",
        "\n",
        "    posting_list_1 = posting_list.pop(0)\n",
        "    posting_list_2 = posting_list.pop(0)\n",
        "\n",
        "    operator = operator_list.pop(0)\n",
        "\n",
        "    # removing the whitespaces\n",
        "    operator = operator.strip()\n",
        "\n",
        "    intermediate_posting_list = []\n",
        "\n",
        "    # Operations on posting list\n",
        "    if operator.lower() == 'and':\n",
        "      intermediate_posting_list = intersection_operation(posting_list_1, posting_list_2)\n",
        "    elif operator.lower() == \"or\":\n",
        "      intermediate_posting_list = union_operation(posting_list_1, posting_list_2)\n",
        "    elif operator.lower() == \"and not\":\n",
        "      not_posting_list = difference_operation(posting_list_2)\n",
        "      intermediate_posting_list = intersection_operation(posting_list_1, not_posting_list)\n",
        "    elif operator.lower() == \"or not\":\n",
        "      not_posting_list = difference_operation(posting_list_2)\n",
        "      intermediate_posting_list = intersection_operation(posting_list_1, not_posting_list)\n",
        "\n",
        "    # appending the intermediate posting list at the 0th index\n",
        "    posting_list.insert(0, intermediate_posting_list)\n",
        "\n",
        "  return posting_list[0]"
      ],
      "metadata": {
        "id": "jK27ZIxZEkoJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the pickled file\n",
        "with open(pkl_file_path, 'rb') as file:\n",
        "  inverted_index = pickle.load(file)\n",
        "\n",
        "# input query\n",
        "print(\"Enter input sequence: \")\n",
        "query = input()\n",
        "\n",
        "# input operations\n",
        "print(\"Enter operations (AND, OR, AND NOT, OR NOT) seperated by comams:\")\n",
        "operations = input()\n",
        "operators = operations.split(',')\n",
        "\n",
        "# passing the query for preprocessing\n",
        "query_tokens = preprocessing(query)\n",
        "\n",
        "print(\"Preprocessed Tokens: \" ,query_tokens)\n",
        "print(\"Operations sequence: \", operators)\n",
        "\n",
        "# Check point 1: Length of the token list is not 0\n",
        "if len(query_tokens) == 0:\n",
        "  print(\"Not a good query\")\n",
        "\n",
        "# Check point 2: If the there is only one token in preprocessed token list\n",
        "elif len(query_tokens)  == 1:\n",
        "  print(\"Query : \", query_tokens[0])\n",
        "\n",
        "# Check point 3: operator list should be 1 less than token list for the operations\n",
        "elif len(query_tokens) -1 != len(operators):\n",
        "  print(\"Not a good operations sequence\")\n",
        "\n",
        "# Check point 4: other wise perform the operations\n",
        "else:\n",
        "  retrieved_document = operations_on_query(query_tokens, operators, inverted_index)\n",
        "  print(\"Retrieved documents: \", retrieved_document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOkleXUMIcAE",
        "outputId": "79ca5591-9081-409d-f05f-a2bb73a8c040"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter input sequence: \n",
            "car bag\n",
            "Enter operations (AND, OR, AND NOT, OR NOT) seperated by comams:\n",
            "and\n",
            "Preprocessed Tokens:  ['car', 'bag']\n",
            "Operations sequence:  ['and']\n",
            "car :  ['file542.txt', 'file174.txt', 'file264.txt', 'file746.txt', 'file886.txt', 'file166.txt']\n",
            "bag :  ['file797.txt', 'file956.txt', 'file174.txt', 'file942.txt', 'file73.txt', 'file892.txt', 'file698.txt', 'file981.txt', 'file313.txt', 'file780.txt', 'file682.txt', 'file738.txt', 'file864.txt', 'file863.txt', 'file573.txt', 'file860.txt', 'file459.txt', 'file404.txt', 'file886.txt', 'file3.txt', 'file118.txt', 'file686.txt', 'file699.txt', 'file466.txt', 'file665.txt', 'file363.txt', 'file930.txt']\n",
            "Operator:  and\n",
            "Posting list 1:  ['file542.txt', 'file174.txt', 'file264.txt', 'file746.txt', 'file886.txt', 'file166.txt']\n",
            "Posting list 2:  ['file797.txt', 'file956.txt', 'file174.txt', 'file942.txt', 'file73.txt', 'file892.txt', 'file698.txt', 'file981.txt', 'file313.txt', 'file780.txt', 'file682.txt', 'file738.txt', 'file864.txt', 'file863.txt', 'file573.txt', 'file860.txt', 'file459.txt', 'file404.txt', 'file886.txt', 'file3.txt', 'file118.txt', 'file686.txt', 'file699.txt', 'file466.txt', 'file665.txt', 'file363.txt', 'file930.txt']\n",
            "Retrieved documents:  [174, 886]\n"
          ]
        }
      ]
    }
  ]
}