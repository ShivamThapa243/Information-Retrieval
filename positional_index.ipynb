{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UnYEeocQ97-Z6IXD-b5PRtdwyx9AFZ1g",
      "authorship_tag": "ABX9TyOqpfYesHf68o08yBsZsa8e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivamThapa243/Information-Retrieval/blob/main/positional_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Inverted Index"
      ],
      "metadata": {
        "id": "uf3b75i_GyaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUze42NYDnpY",
        "outputId": "83ccb85d-5734-4ebd-eddf-fb884cdc01c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# importing dataset from drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Point: Checking if the files are readable"
      ],
      "metadata": {
        "id": "XdiZptKUHT06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accessing the dattaset\n",
        "path = \"/content/drive/MyDrive/Dataset/text_files/file1.txt\"\n",
        "with open(path, 'r') as file:\n",
        "  content = file.read()\n",
        "\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYhURooYGOVb",
        "outputId": "9e9b5c48-6f11-4974-fb6f-393703e54ffa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function definitions (include preprocessing functions)**"
      ],
      "metadata": {
        "id": "L9rC0MdupFwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(content):\n",
        "  # 1. converting the string into lowercase and removing meta-tags\n",
        "  content = content.lower()\n",
        "  content = metadata_remover(content)\n",
        "  # 2. converting string into tokens\n",
        "  token_array = tokenizer_function(content)\n",
        "  # 3. removing stopwords\n",
        "  token_array = stopwords_remover_funtion(token_array)\n",
        "  # 4. Punctuation remover function\n",
        "  token_array = punctuation_remover_function(token_array)\n",
        "  # 5. converting token array to string,\n",
        "  # so that it can be stored as a string in the text file\n",
        "  preprocessed_tokens = stringfy_token_array(token_array)\n",
        "  return preprocessed_tokens\n",
        "# end of functtion\n",
        "\n",
        "\n",
        "# function to convert the string into lower case and no meta data\n",
        "def metadata_remover(content):\n",
        "  tag = False\n",
        "  quote = False\n",
        "  str = \"\"\n",
        "  for ch in content:\n",
        "    if ch == '<' and not quote:\n",
        "      tag = True\n",
        "    elif ch == '>' and not quote:\n",
        "      tag = False\n",
        "    elif (ch == '\"' or ch == \"'\") and tag:\n",
        "      quote = not quote\n",
        "    elif not tag:\n",
        "      str = str + ch\n",
        "\n",
        "  return str\n",
        "# end of function\n",
        "\n",
        "# function to convert string into tokens\n",
        "def tokenizer_function(content):\n",
        "  token_array = content.split()\n",
        "  return token_array\n",
        "# end of function\n",
        "\n",
        "# funtion to remove stopwords from the token array\n",
        "def stopwords_remover_funtion(token_array):\n",
        "  stopwords = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
        "                 \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
        "                 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
        "                 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is',\n",
        "                 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
        "                 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
        "                 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
        "                 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
        "                 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n",
        "                 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
        "                 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\",\n",
        "                 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn',\n",
        "                 \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
        "                 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn',\n",
        "                 \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"}\n",
        "  token_array = []\n",
        "  for token in token_array:\n",
        "    if token in stopwords:\n",
        "      continue\n",
        "    else:\n",
        "      token_array.append(token)\n",
        "  return token_array\n",
        "# end of funtion\n",
        "\n",
        "# funtion to remove punctuation\n",
        "def punctuation_remover_function(token_array):\n",
        "  new_token = []\n",
        "  puntuation = {'.', '?', '!', ',', ';', ':', '\\'', '\\\"', '-', '_', ')', '(', '[', ']', '{', '}', '<', '>', '=', '/', '\\\\', '*', '#'}\n",
        "  for token in token_array:\n",
        "    temp = \"\"\n",
        "    for ch in token:\n",
        "      if  ch in punctuation:\n",
        "        continue\n",
        "      else:\n",
        "        temp += ch\n",
        "    new_token.append(temp)\n",
        "  return new_token\n",
        "#end of function\n",
        "\n",
        "# function to convert token array into string\n",
        "def stringfy_token_array(token_array):\n",
        "  token_string = \"\"\n",
        "  for str in token_array:\n",
        "    token_string += str\n",
        "    token_string += \" \"\n",
        "  return token_string\n",
        "# end of function"
      ],
      "metadata": {
        "id": "Kw3QIKqapFT9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing the file content and storing in a new directory**"
      ],
      "metadata": {
        "id": "_bi24OpNBHrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory_path = \"/content/drive/MyDrive/Dataset\"\n",
        "preprocessed_directory = \"/content/drive/MyDrive/Preprocessed-Dataset\"\n",
        "# Create the preprocessed directory if it doesn't exist\n",
        "os.makedirs(preprocessed_directory, exist_ok= True)\n",
        "\n",
        "# List all files in the directory, excluding subdirectories\n",
        "# file_list = os.listdir(directory_path)\n",
        "file_list = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))] # Filter out directories\n",
        "\n",
        "print(f\"Files to process: {file_list}\")\n",
        "\n",
        "# iterating through the dataset and preprocessing each file present in the directory\n",
        "for filename in file_list:\n",
        "  file_path = os.path.join(directory_path, filename)\n",
        "  # reading the original content of the dataset\n",
        "  with open(file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "  print(\"Original content : \", content)\n",
        "  # preprocessing the original document\n",
        "  preprocessed_content = preprocessing(content)\n",
        "\n",
        "  print(\"Preprocessed Content : \", preprocessed_content, \"\\n\")\n",
        "\n",
        "  # writing/ storing the preprocessed_content into the newly created directory\n",
        "  preprocessed_file_path = os.path.join(preprocessed_directory, filename)\n",
        "  with open(preprocessed_file_path, 'w') as preprocessed_file:\n",
        "    preprocessed_file.write(preprocessed_content)\n",
        "#end of for loop"
      ],
      "metadata": {
        "id": "xTQzUeplnIli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a4781b-e96f-4f12-f063-b05a48e83245"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files to process: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing 5 Original documents before and after preprocessing"
      ],
      "metadata": {
        "id": "gOqoGDkb_jph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_directory_path = \"/content/Preprocessed-Dataset\"\n",
        "preprocessed_file_list = os.listdir(preprocessed_directory_path)\n",
        "\n",
        "file_count = 1\n",
        "\n",
        "for filename in preprocessed_file_list:\n",
        "  if file_count > 5:\n",
        "    break\n",
        "\n",
        "  preprocessed_file_path = os.path.join(preprocessed_directory_path, filename)\n",
        "  with open(preprocessed_file_path, 'r') as file:\n",
        "    preprocessed_content = file.read()\n",
        "\n",
        "  print(file_count, f\" {filename}\")\n",
        "  print(preprocessed_content, \"\\n\")\n",
        "  file_count += 1"
      ],
      "metadata": {
        "id": "lwmSr5DD_hxy"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}